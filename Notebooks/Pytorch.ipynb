{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea6eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "#from charlm.ngram import CharNGram\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# torch.set_printoptions(sci_mode=False, precision=5)\n",
    "# torch.set_default_device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd9e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../charlm\"))\n",
    "from ngram import CharNGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "160f730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(\"../data/input/dog_names.txt\", \"r\") as f:\n",
    "    names = list(set([i for i in f.read().splitlines()]))\n",
    "\n",
    "names_train = random.sample(names, k=int(0.9*(len(names))), )\n",
    "names_test = list(set(names) - set(names_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f66e08",
   "metadata": {},
   "source": [
    "# N-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a97318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss (train) n-gram LM: 2.50610\n",
      "Log loss (test) n-gram LM: 2.51029\n"
     ]
    }
   ],
   "source": [
    "n_gram_size = 2\n",
    "ngram_dog_names = CharNGram(size=n_gram_size, smoothing_factor=1)\n",
    "ngram_dog_names.fit(names_train, names_test, loss_type = \"cross_entropy\", min_probability=0)\n",
    "loss_train = ngram_dog_names.loss[\"train\"]\n",
    "loss_test = ngram_dog_names.loss[\"test\"]\n",
    "print(f\"Log loss (train) n-gram LM: {loss_train:.5f}\")\n",
    "print(f\"Log loss (test) n-gram LM: {loss_test:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadd403",
   "metadata": {},
   "source": [
    "# Neural language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dfc0ecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1000/1000 [00:05<00:00, 168.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss (train) n-gram neural LM: 2.50436\n",
      "Log loss (test) n-gram neural LM: 2.50893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_gram_size = 2\n",
    "ngram_dog_names_nn = CharNGram(size=n_gram_size, estimation_method=\"nn\")\n",
    "ngram_dog_names_nn.fit(train=names_train, test=names_test,learning_rate=50, epochs=1000, \n",
    "                       loss_type=\"cross_entropy\", device=\"cpu\")\n",
    "train_loss = ngram_dog_names_nn.loss[\"train\"]\n",
    "test_loss = ngram_dog_names_nn.loss[\"test\"]\n",
    "print(f\"Log loss (train) n-gram neural LM: {train_loss:.5f}\")\n",
    "print(f\"Log loss (test) n-gram neural LM: {test_loss:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charlm",
   "language": "python",
   "name": "charlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
