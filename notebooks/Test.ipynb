{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db0945e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51dc591",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a84939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/dog_names.txt\", \"r\") as f:\n",
    "    names = list(set([i for i in f.read().splitlines()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d43095fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_train = random.sample(names, k=int(0.95*(len(names))))\n",
    "names_test = list(set(names) - set(names_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece5600",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45c848d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_list(list_object, n):\n",
    "    ngrams = []\n",
    "    for element in list_object:\n",
    "        if len(element)>=n:\n",
    "            ngrams.append((\"<>\" + element[0:n-2], element[n-2]))\n",
    "            for idx in range(len(element) - n + 1):\n",
    "                ngrams.append((element[idx:idx+n-1], element[idx+n-1]))\n",
    "            ngrams.append((element[len(element)-n+1:], \"<>\"))\n",
    "    return ngrams\n",
    "\n",
    "def ngram_list_updated(list_object, n):\n",
    "    ngrams = []\n",
    "    for n_of_grams in range(2, n+1):\n",
    "        for element in list_object:\n",
    "            element = [\"<>\"] + list(element) + [\"<>\"]\n",
    "            if len(element)>=(n_of_grams-2):\n",
    "                for idx in range(len(element) - n_of_grams + 1):\n",
    "                    ngrams.append((\"\".join(element[idx:idx+n_of_grams-1]), \"\".join(element[idx+n_of_grams-1])))\n",
    "    return ngrams\n",
    "\n",
    "def ngram_count(list_ngrams):\n",
    "    ngrams_counts = {}\n",
    "    for ngram in list_ngrams:\n",
    "        if ngram in ngrams_counts:\n",
    "            ngrams_counts[ngram] += 1\n",
    "        else:\n",
    "            ngrams_counts[ngram] = 1\n",
    "    return ngrams_counts\n",
    "\n",
    "def calculate_conditional_probabilities(ngram_counts):\n",
    "    firsts = sorted(list(set([i[0] for i in ngram_counts.keys()])))\n",
    "    nexts = sorted(list(set([i[1] for i in ngram_counts.keys()])))\n",
    "    probabilities = np.zeros((len(firsts), len(nexts)))\n",
    "    for idx_f, f in enumerate(firsts):\n",
    "        for idx_n, n in enumerate(nexts):\n",
    "            probabilities[(idx_f, idx_n)] = counts.get((f, n), 0)\n",
    "    probabilities = probabilities / probabilities.sum(axis=1, keepdims=True)\n",
    "    return firsts, nexts, probabilities\n",
    "\n",
    "def generate_word(firsts, nexts, probabilites, n):\n",
    "    first_char = str(np.random.choice(nexts, size=1, replace=True, p=probabilities[firsts.index(\"<>\")])[0])\n",
    "    word = first_char\n",
    "    while True:\n",
    "        prev_ngram = word[-(n-1):] if len(word)>=(n-1) else '<>'+word\n",
    "        next_char = str(np.random.choice(nexts, size=1, replace=True, p=probabilities[firsts.index(prev_ngram)])[0])\n",
    "        if next_char == \"<>\":\n",
    "            break\n",
    "        word += next_char\n",
    "    return word\n",
    "\n",
    "def generate_words(n_words, firsts, nexts, probabilities, n):\n",
    "    words = []\n",
    "    for i in range(n_words):\n",
    "        words.append(generate_word(firsts, nexts, probabilities, n))\n",
    "    return words\n",
    "\n",
    "def calculate_perplexity(word, probabilities, n):\n",
    "    word = [\"<>\"] + list(word) + [\"<>\"]\n",
    "    predictor_grams = []\n",
    "    for idx_char, char in enumerate(word[:-1]):\n",
    "        predictor_grams.append(\"\".join(word[max(0, idx_char - (n-1) + 1):idx_char+1]))\n",
    "    perplexity = 1\n",
    "    for predictor, test in zip(predictor_grams, word[1:]):\n",
    "        try:\n",
    "            probability = float(probabilities[firsts.index(predictor)][nexts.index(test)])\n",
    "            probability = probability if probability>0 else 0.001\n",
    "        except:\n",
    "            probability = 1\n",
    "        perplexity *= (probability)**(-1/len(predictor_grams))\n",
    "    return perplexity\n",
    "\n",
    "def calculate_test_set_perplexities(test_list, probabilities, n):\n",
    "    perplexities = []\n",
    "    for element in test_list:\n",
    "        perplexities.append(calculate_perplexity(element, probabilities, n))\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b4abfb",
   "metadata": {},
   "source": [
    "# Test set perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7ad0ff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total perplexity: 6098.503465321157\n",
      "Mean perplexity: 12.838954663834015\n"
     ]
    }
   ],
   "source": [
    "# Test set perplexity\n",
    "n = 6\n",
    "ngrams = ngram_list_updated(names_train, n)\n",
    "counts= ngram_count(ngrams)\n",
    "firsts, nexts, probabilities = calculate_conditional_probabilities(counts)\n",
    "perplexities = calculate_test_set_perplexities(names_test, probabilities, n)\n",
    "print(f\"Total perplexity: {sum(perplexities)}\")\n",
    "print(f\"Mean perplexity: {sum(perplexities)/len(perplexities)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaf819d",
   "metadata": {},
   "source": [
    "# Word generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "37746dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of words generated that are also in train set: 0.9307036247334755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = generate_words(1000, firsts, nexts, probabilities, n)\n",
    "print(f\"% of words generated that are also in train set: {len(set(words).intersection(set(names_train)))/len(set(words))}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "96b262a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of new words: charella, clementin, lionhearty, chambar, harastro, yolandi, agnolia, raubautzi, ottavian, carnell\n"
     ]
    }
   ],
   "source": [
    "new_names = \", \".join(list(set(words) - set(names_train))[0:10])\n",
    "print(f\"Examples of new words: {new_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c63a50c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['magnolia']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in names if \"agnolia\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7ac13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charlm",
   "language": "python",
   "name": "charlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
