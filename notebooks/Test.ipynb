{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "db0945e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab5ab0",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e6ef7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/dog_names.txt\", \"r\") as f:\n",
    "    names = list(set([i for i in f.read().splitlines()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "75e6fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_train = random.sample(names, k=int(0.95*(len(names))))\n",
    "names_set = list(set(names) - set(names_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047edb3",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "76d2b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_list(list_object, n):\n",
    "    ngrams = []\n",
    "    for element in list_object:\n",
    "        if len(element)>=n:\n",
    "            ngrams.append((\"<>\" + element[0:n-2], element[n-2]))\n",
    "            for idx in range(len(element) - n + 1):\n",
    "                ngrams.append((element[idx:idx+n-1], element[idx+n-1]))\n",
    "            ngrams.append((element[len(element)-n+1:], \"<>\"))\n",
    "    return ngrams\n",
    "\n",
    "def ngram_list_updated(list_object, n):\n",
    "    ngrams = []\n",
    "    for n_of_grams in range(2, n+1):\n",
    "        for element in list_object:\n",
    "            element = [\"<>\"] + list(element) + [\"<>\"]\n",
    "            if len(element)>=(n_of_grams-2):\n",
    "                for idx in range(len(element) - n_of_grams + 1):\n",
    "                    ngrams.append((\"\".join(element[idx:idx+n_of_grams-1]), \"\".join(element[idx+n_of_grams-1])))\n",
    "    return ngrams\n",
    "\n",
    "def ngram_count(list_ngrams):\n",
    "    ngrams_counts = {}\n",
    "    for ngram in list_ngrams:\n",
    "        if ngram in ngrams_counts:\n",
    "            ngrams_counts[ngram] += 1\n",
    "        else:\n",
    "            ngrams_counts[ngram] = 1\n",
    "    return ngrams_counts\n",
    "\n",
    "def calculate_conditional_probabilities(ngram_counts):\n",
    "    firsts = sorted(list(set([i[0] for i in ngram_counts.keys()])))\n",
    "    nexts = sorted(list(set([i[1] for i in ngram_counts.keys()])))\n",
    "    probabilities = np.zeros((len(firsts), len(nexts)))\n",
    "    for idx_f, f in enumerate(firsts):\n",
    "        for idx_n, n in enumerate(nexts):\n",
    "            probabilities[(idx_f, idx_n)] = counts.get((f, n), 0)\n",
    "    probabilities = probabilities / probabilities.sum(axis=1, keepdims=True)\n",
    "    return firsts, nexts, probabilities\n",
    "\n",
    "def generate_word(firsts, nexts, probabilites, n):\n",
    "    first_char = str(np.random.choice(nexts, size=1, replace=True, p=probabilities[firsts.index(\"<>\")])[0])\n",
    "    word = first_char\n",
    "    while True:\n",
    "        prev_ngram = word[-(n-1):] if len(word)>=(n-1) else '<>'+word\n",
    "        next_char = str(np.random.choice(nexts, size=1, replace=True, p=probabilities[firsts.index(prev_ngram)])[0])\n",
    "        if next_char == \"<>\":\n",
    "            break\n",
    "        word += next_char\n",
    "    return word\n",
    "\n",
    "def generate_words(n_words, firsts, nexts, probabilities, n):\n",
    "    words = []\n",
    "    for i in range(n_words):\n",
    "        words.append(generate_word(firsts, nexts, probabilities, n))\n",
    "    return words\n",
    "\n",
    "def calculate_perplexity(word, probabilities, n):\n",
    "    word = [\"<>\"] + list(word) + [\"<>\"]\n",
    "    predictor_grams = []\n",
    "    for idx_char, char in enumerate(word[:-1]):\n",
    "        predictor_grams.append(\"\".join(word[max(0, idx_char - (n-1) + 1):idx_char+1]))\n",
    "    perplexity = 1\n",
    "    for predictor, test in zip(predictor_grams, word[1:]):\n",
    "        try:\n",
    "            probability = float(probabilities[firsts.index(predictor)][nexts.index(test)])\n",
    "            probability = probability if probability>0 else 0.001\n",
    "        except:\n",
    "            probability = 1\n",
    "        perplexity *= (probability)**(-1/len(predictor_grams))\n",
    "    return perplexity\n",
    "\n",
    "def calculate_test_set_perplexities(test_list, probabilities, n):\n",
    "    perplexities = []\n",
    "    for element in test_list:\n",
    "        perplexities.append(calculate_perplexity(element, probabilities, n))\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "a30d334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "ngrams = ngram_list_updated(names_train, n)\n",
    "counts= ngram_count(ngrams)\n",
    "firsts, nexts, probabilities = calculate_conditional_probabilities(counts)\n",
    "words = generate_words(1000, firsts, nexts, probabilities, n)\n",
    "perplexities = calculate_test_set_perplexities(names_test, probabilities, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "c5232156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2762.8904731790235"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "4501ea5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6805845511482255"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(words).intersection(set(names_train)))/len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "909de858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'achilin',\n",
       " 'adaschan',\n",
       " 'adellek',\n",
       " 'admiranhatty',\n",
       " 'adventu',\n",
       " 'airi',\n",
       " 'ajos',\n",
       " 'alado',\n",
       " 'aladonny',\n",
       " 'alettylos',\n",
       " 'alfa',\n",
       " 'alino',\n",
       " 'alis',\n",
       " 'amely',\n",
       " 'amero',\n",
       " 'anacho',\n",
       " 'anessinja',\n",
       " 'anti',\n",
       " 'aphrates',\n",
       " 'aquarry',\n",
       " 'aranit',\n",
       " 'ardo',\n",
       " 'areni',\n",
       " 'arica',\n",
       " 'aristy',\n",
       " 'arnaby',\n",
       " 'arnet',\n",
       " 'artie',\n",
       " 'asha',\n",
       " 'asheresa',\n",
       " 'asmar',\n",
       " 'assios',\n",
       " 'auringo',\n",
       " 'ayukarin',\n",
       " 'babetty',\n",
       " 'baccaramba',\n",
       " 'bacimar',\n",
       " 'baffi',\n",
       " 'bagney',\n",
       " 'balerina',\n",
       " 'banglei',\n",
       " 'basugar',\n",
       " 'bikola',\n",
       " 'boomeron',\n",
       " 'bootsie',\n",
       " 'branke',\n",
       " 'braxas',\n",
       " 'cagnan',\n",
       " 'calet',\n",
       " 'camilena',\n",
       " 'campire',\n",
       " 'candia',\n",
       " 'caralie',\n",
       " 'carrel',\n",
       " 'cassima',\n",
       " 'chakom',\n",
       " 'chamberlanagan',\n",
       " 'charlon',\n",
       " 'charlyn',\n",
       " 'charontus',\n",
       " 'chestina',\n",
       " 'chiccos',\n",
       " 'chie',\n",
       " 'chino',\n",
       " 'chocoletto',\n",
       " 'chubble',\n",
       " 'ciester',\n",
       " 'cintill',\n",
       " 'corally',\n",
       " 'cordono',\n",
       " 'coustaaf',\n",
       " 'cress',\n",
       " 'crick',\n",
       " 'daphire',\n",
       " 'desper',\n",
       " 'diamarcon',\n",
       " 'disantus',\n",
       " 'divanna',\n",
       " 'domington',\n",
       " 'dono',\n",
       " 'dore',\n",
       " 'dreamweavera',\n",
       " 'egilia',\n",
       " 'ellini',\n",
       " 'elody',\n",
       " 'emera',\n",
       " 'erista',\n",
       " 'ernestalia',\n",
       " 'esca',\n",
       " 'esch',\n",
       " 'fanda',\n",
       " 'fargon',\n",
       " 'farlee',\n",
       " 'favoriella',\n",
       " 'ferron',\n",
       " 'fidelios',\n",
       " 'fidibur',\n",
       " 'filkos',\n",
       " 'fines',\n",
       " 'fioretti',\n",
       " 'flamila',\n",
       " 'flavin',\n",
       " 'fletchen',\n",
       " 'florengel',\n",
       " 'friartus',\n",
       " 'frisch',\n",
       " 'galador',\n",
       " 'gall',\n",
       " 'gallodri',\n",
       " 'garellar',\n",
       " 'geister',\n",
       " 'gille',\n",
       " 'ginnah',\n",
       " 'goranija',\n",
       " 'gordille',\n",
       " 'granda',\n",
       " 'gromika',\n",
       " 'guineverly',\n",
       " 'gustus',\n",
       " 'hadelle',\n",
       " 'hadescheiner',\n",
       " 'haggardo',\n",
       " 'heid',\n",
       " 'hell',\n",
       " 'hendra',\n",
       " 'hennifer',\n",
       " 'hermin',\n",
       " 'herrmllerina',\n",
       " 'hershwin',\n",
       " 'hubby',\n",
       " 'huber',\n",
       " 'hughetta',\n",
       " 'husch',\n",
       " 'ibri',\n",
       " 'ilan',\n",
       " 'ilis',\n",
       " 'ilkan',\n",
       " 'indi',\n",
       " 'innober',\n",
       " 'irresista',\n",
       " 'ivorika',\n",
       " 'jakin',\n",
       " 'jakita',\n",
       " 'janjana',\n",
       " 'jaschi',\n",
       " 'jerro',\n",
       " 'jersepha',\n",
       " 'jessi',\n",
       " 'junionja',\n",
       " 'jupitero',\n",
       " 'kabor',\n",
       " 'kallini',\n",
       " 'kalma',\n",
       " 'kantom',\n",
       " 'karne',\n",
       " 'karus',\n",
       " 'kassi',\n",
       " 'kerrypie',\n",
       " 'kies',\n",
       " 'kingoldine',\n",
       " 'kiros',\n",
       " 'klothar',\n",
       " 'knechter',\n",
       " 'korsar',\n",
       " 'krisch',\n",
       " 'kristo',\n",
       " 'kron',\n",
       " 'kuschuk',\n",
       " 'lamba',\n",
       " 'lant',\n",
       " 'lanuschka',\n",
       " 'lanzai',\n",
       " 'lashback',\n",
       " 'latiscilly',\n",
       " 'laude',\n",
       " 'lemona',\n",
       " 'lenna',\n",
       " 'lest',\n",
       " 'lischa',\n",
       " 'lolipo',\n",
       " 'loredance',\n",
       " 'luka',\n",
       " 'lumps',\n",
       " 'lussie',\n",
       " 'mabusa',\n",
       " 'machillio',\n",
       " 'maggi',\n",
       " 'malexis',\n",
       " 'mancha',\n",
       " 'marqueeny',\n",
       " 'marshalina',\n",
       " 'marus',\n",
       " 'matadore',\n",
       " 'menoso',\n",
       " 'meralda',\n",
       " 'mesquitter',\n",
       " 'mile',\n",
       " 'mindar',\n",
       " 'mine',\n",
       " 'minoresto',\n",
       " 'mirkoro',\n",
       " 'missme',\n",
       " 'nadjanka',\n",
       " 'naspar',\n",
       " 'natalin',\n",
       " 'neress',\n",
       " 'nesch',\n",
       " 'nesse',\n",
       " 'nicane',\n",
       " 'nickett',\n",
       " 'nicolonso',\n",
       " 'nicost',\n",
       " 'nore',\n",
       " 'nuel',\n",
       " 'ociaociana',\n",
       " 'odysse',\n",
       " 'okai',\n",
       " 'okirala',\n",
       " 'ondo',\n",
       " 'ongle',\n",
       " 'oogabor',\n",
       " 'oranira',\n",
       " 'oska',\n",
       " 'ossirius',\n",
       " 'pandelissila',\n",
       " 'pandia',\n",
       " 'perl',\n",
       " 'pernos',\n",
       " 'pippie',\n",
       " 'pitte',\n",
       " 'pygmalo',\n",
       " 'quarissy',\n",
       " 'querlinda',\n",
       " 'quichelsy',\n",
       " 'quirlie',\n",
       " 'raff',\n",
       " 'rani',\n",
       " 'raolito',\n",
       " 'raston',\n",
       " 'razie',\n",
       " 'regino',\n",
       " 'rick',\n",
       " 'rigger',\n",
       " 'ring',\n",
       " 'rosarius',\n",
       " 'rosina',\n",
       " 'sabo',\n",
       " 'sador',\n",
       " 'samity',\n",
       " 'sandros',\n",
       " 'saredolina',\n",
       " 'sarius',\n",
       " 'saskis',\n",
       " 'scha',\n",
       " 'schakon',\n",
       " 'schi',\n",
       " 'schnu',\n",
       " 'schoice',\n",
       " 'scoobydick',\n",
       " 'selmar',\n",
       " 'senecia',\n",
       " 'sergi',\n",
       " 'shamrocke',\n",
       " 'shantom',\n",
       " 'sharrago',\n",
       " 'sheily',\n",
       " 'sher',\n",
       " 'sokrate',\n",
       " 'sonner',\n",
       " 'spence',\n",
       " 'stro',\n",
       " 'sussuf',\n",
       " 'takkordo',\n",
       " 'tallus',\n",
       " 'tamandy',\n",
       " 'tamiral',\n",
       " 'terenia',\n",
       " 'termia',\n",
       " 'termingo',\n",
       " 'tinseng',\n",
       " 'titano',\n",
       " 'tracer',\n",
       " 'trouba',\n",
       " 'trufflo',\n",
       " 'twinnis',\n",
       " 'ushanty',\n",
       " 'valentice',\n",
       " 'vallett',\n",
       " 'vana',\n",
       " 'varastaway',\n",
       " 'vernake',\n",
       " 'virgilli',\n",
       " 'walpur',\n",
       " 'waterlos',\n",
       " 'wenny',\n",
       " 'whistock',\n",
       " 'widget',\n",
       " 'wildthinka',\n",
       " 'winer',\n",
       " 'winny',\n",
       " 'wood',\n",
       " 'wummer',\n",
       " 'yentos',\n",
       " 'zankor',\n",
       " 'zepellis',\n",
       " 'zhirade'}"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(words) - set(names_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9090581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charlm",
   "language": "python",
   "name": "charlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
